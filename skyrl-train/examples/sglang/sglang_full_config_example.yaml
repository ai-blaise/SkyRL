# SkyRL + SGLang Full Configuration Example
#
# This file demonstrates all available SGLang-specific configuration options.
# Copy and modify for your use case.
#
# Usage:
#   uv run --extra sglang -m skyrl_train.entrypoints.main_base \
#     --config-path examples/sglang \
#     --config-name sglang_full_config_example

defaults:
  - /ppo_base_config
  - _self_

# =============================================================================
# TRAINER CONFIGURATION
# =============================================================================
trainer:
  strategy: fsdp2  # or "megatron" (experimental with SGLang)

  placement:
    colocate_all: true  # Enable for CUDA IPC weight sync + sleep/wake

  policy:
    model:
      path: "Qwen/Qwen2.5-7B-Instruct"
    learning_rate: 1e-6

  critic:
    model:
      path: "Qwen/Qwen2.5-7B-Instruct"
    learning_rate: 5e-6

  ref:
    model:
      path: "Qwen/Qwen2.5-7B-Instruct"

  algorithm:
    adv_estimator: grpo  # gae, grpo, rloo, reinforce_plus_plus
    policy_loss: regular  # regular, gspo, sapo
    eps_clip_low: 0.2
    eps_clip_high: 0.2
    kl_coef: 0.0
    entropy_coef: 0.0

# =============================================================================
# GENERATOR (INFERENCE) CONFIGURATION
# =============================================================================
generator:
  # ---------------------------------------------------------------------------
  # Backend Selection
  # ---------------------------------------------------------------------------
  backend: sglang  # Use SGLang as inference backend

  # ---------------------------------------------------------------------------
  # Engine Instance Configuration
  # ---------------------------------------------------------------------------
  num_inference_engines: 4  # Number of parallel SGLang engine instances
  run_engines_locally: true  # Run engines in same Ray cluster

  # ---------------------------------------------------------------------------
  # Parallelism Settings (per engine)
  # ---------------------------------------------------------------------------
  inference_engine_tensor_parallel_size: 2   # GPUs per engine for TP
  inference_engine_pipeline_parallel_size: 1  # Pipeline stages
  inference_engine_data_parallel_size: 1      # Data parallel replicas
  inference_engine_expert_parallel_size: 1    # Expert parallelism (MoE only)

  # ---------------------------------------------------------------------------
  # Memory Configuration
  # ---------------------------------------------------------------------------
  gpu_memory_utilization: 0.85  # Fraction of GPU memory to use
  max_num_batched_tokens: 8192  # Max tokens in prefill batch
  max_num_seqs: 1024            # Max concurrent sequences

  # ---------------------------------------------------------------------------
  # Caching & Performance
  # ---------------------------------------------------------------------------
  enable_prefix_caching: true  # RadixAttention for prefix caching
  async_engine: true           # Use async generation
  batched: true                # Enable batched generation

  # ---------------------------------------------------------------------------
  # Weight Synchronization
  # ---------------------------------------------------------------------------
  weight_sync_backend: nccl  # nccl (recommended) or gloo
  # When colocate_all=true + nccl: Uses CUDA IPC (zero-copy)
  # Otherwise: Uses torch.distributed broadcast

  # Overlapped weight sync: transfer weights in background while generation continues
  # Reduces pause time during weight sync from full transfer time to just application time
  use_overlapped_weight_sync: false  # Set true for faster weight sync with SGLang

  # ---------------------------------------------------------------------------
  # Deterministic Inference (for on-policy RL reproducibility)
  # ---------------------------------------------------------------------------
  # Enables reproducible inference for true on-policy RL training.
  # Trade-off: ~34% performance overhead, but CUDA graphs provide 2.8x speedup.
  #
  # RECOMMENDED FOR ON-POLICY RL (PPO, GRPO):
  #   enabled: true
  #   rl_on_policy_target: "fsdp"
  #
  # For reproducible non-greedy sampling, also set seed in sampling_params.
  deterministic_inference:
    # Enable deterministic inference mode
    # Uses batch-invariant kernels and fixed split-KV sizes
    enabled: false  # Set to true for on-policy RL

    # On-policy target backend alignment for zero KL divergence
    # Options: null (disabled), "fsdp" (match FSDP training backend)
    # When "fsdp": uses native PyTorch ops, computes rotary embeddings on CPU
    # RECOMMENDED: "fsdp" for on-policy RL training
    rl_on_policy_target: null  # Set to "fsdp" for on-policy RL

  # ---------------------------------------------------------------------------
  # FP8 KV Cache (optional, ~50% memory reduction)
  # ---------------------------------------------------------------------------
  # Enables FP8 quantization for KV cache, reducing memory by ~50%
  # Requires CUDA 11.8+ and compatible GPU (Hopper/Blackwell recommended)
  kv_cache:
    # KV cache data type
    # - "auto": Uses model data type (default)
    # - "fp8_e4m3": FP8 with 4-bit exponent, 3-bit mantissa (recommended for FP8)
    #              Note: FA3 only supports fp8_e4m3; fp8_e5m2 falls back to triton
    # - "fp8_e5m2": FP8 with 5-bit exponent, 2-bit mantissa
    dtype: "auto"

    # Path to JSON file containing KV cache scaling factors
    # Highly recommended when using FP8 to avoid accuracy degradation
    # If not provided, defaults to 1.0 scaling (may reduce accuracy)
    quantization_param_path: null

    # FP8 GEMM runner backend (affects FP8 matrix multiplication performance)
    # - "auto": Auto-select based on hardware (default)
    # - "deep_gemm": JIT-compiled, optimal for Hopper (SM90) and Blackwell (SM100)
    # - "cutlass": Optimal for Hopper/Blackwell, high-throughput
    # - "triton": Fallback, widely compatible
    fp8_gemm_backend: "auto"

  # ---------------------------------------------------------------------------
  # Multi-turn Configuration
  # ---------------------------------------------------------------------------
  use_conversation_multi_turn: true  # REQUIRED for SGLang backend

  # ---------------------------------------------------------------------------
  # LoRA Configuration (optional)
  # ---------------------------------------------------------------------------
  enable_lora: false  # Set true to enable LoRA
  # When enabled, also set in engine_init_kwargs:
  #   max_lora_rank: 64
  #   max_loras_per_batch: 8
  #   lora_backend: "csgmv"

  # ---------------------------------------------------------------------------
  # OOM Recovery (built-in)
  # ---------------------------------------------------------------------------
  # SGLang handles OOM via request retraction. SkyRL adds retry logic:
  # - Detects OOM errors (prefill/decode out of memory, CUDA OOM)
  # - Retries up to 3 times with exponential backoff (0.5s, 1s, 2s)
  # To reduce OOM frequency: lower gpu_memory_utilization, max_num_seqs,
  # or increase scheduling.conservativeness

  # ---------------------------------------------------------------------------
  # HTTP Endpoint (optional)
  # ---------------------------------------------------------------------------
  enable_http_endpoint: false  # Set true for OpenAI-compatible API
  http_endpoint_host: "0.0.0.0"
  http_endpoint_port: 8000

  # ---------------------------------------------------------------------------
  # Sampling Parameters
  # ---------------------------------------------------------------------------
  sampling_params:
    max_generate_length: 1024  # Maximum tokens to generate
    temperature: 1.0           # Sampling temperature (>0)
    top_p: 1.0                 # Nucleus sampling threshold (0, 1]
    top_k: -1                  # Top-k sampling (-1 = disabled)
    min_p: 0.0                 # Min probability threshold [0, 1]

    # Penalty parameters
    frequency_penalty: 0.0     # Cumulative frequency penalty [-2, 2]
    presence_penalty: 0.0      # Binary presence penalty [-2, 2]
    repetition_penalty: 1.0    # Multiplicative repetition penalty [0, 2]

    # Generation length constraints
    min_new_tokens: 0          # Minimum tokens to generate

    # Stop conditions
    stop: null                 # Stop strings (converted to token IDs)
    # Example: stop: ["</s>", "\n\n", "```"]

    # Logprobs
    logprobs: 0  # 0 = return logprobs, N = return top N logprobs

  # ---------------------------------------------------------------------------
  # Evaluation Sampling Parameters (for eval runs)
  # ---------------------------------------------------------------------------
  eval_sampling_params:
    max_generate_length: 1024
    temperature: 0.0  # Greedy for evaluation
    top_p: 1.0
    top_k: -1

  # ---------------------------------------------------------------------------
  # Speculative Decoding (optional, 2-3x inference speedup)
  # ---------------------------------------------------------------------------
  # Enables draft model to speculatively predict tokens, then verify with target
  # Note: Not compatible with LoRA (except ngram algorithm)
  speculative_decoding:
    enabled: false  # Set true to enable

    # Algorithm options:
    # - "eagle"/"eagle3": Tree-based with EAGLE-trained draft model (fastest)
    # - "standalone": Separate small model from same family
    # - "ngram": Pattern matching in token history (no draft model, LoRA-compatible)
    algorithm: null  # Required when enabled=true

    # Draft model path (required for eagle/standalone, not for ngram)
    # EAGLE: "yuhuili/EAGLE-LLaMA3-Instruct-8B"
    # Standalone: "meta-llama/Llama-3.2-1B" (smaller model from same family)
    draft_model_path: null

    # Decoding parameters (auto-chosen if null)
    num_steps: null        # Speculative steps (recommended: 3-5)
    eagle_topk: null       # Top-k candidates (topk>1 requires fa3/flashinfer)
    num_draft_tokens: null # Draft tokens per step

    # Acceptance thresholds (1.0 = exact match, lower = more aggressive)
    accept_threshold_single: 1.0
    accept_threshold_acc: 1.0

    # N-gram specific options (only for algorithm="ngram")
    # ngram:
    #   max_match_window_size: 16  # Tuned for RL rollouts (up from 12)
    #   max_bfs_breadth: 15        # Tuned for RL rollouts (up from 10)
    #   match_type: "BFS"  # or "PROB"

  # ---------------------------------------------------------------------------
  # Piecewise CUDA Graphs (optional, for variable-length prefill optimization)
  # ---------------------------------------------------------------------------
  # Piecewise CUDA graphs optimize the prefill/extend phase with variable-length inputs.
  # This is experimental and provides acceleration for the entire forward pass.
  piecewise_cuda_graph:
    # Enable piecewise CUDA graphs for prefill phase optimization
    # Required for disaggregated prefill mode
    enabled: false

    # Maximum token count for piecewise CUDA graph capture
    # Auto-set to chunked_prefill_size if null (default: 4096)
    max_tokens: null

    # Explicit list of token counts to capture
    # If null, auto-generated: [4,8,...,32] + [48,64,...,256] + [288,...,4096] + ...
    token_counts: null

    # Compiler for piecewise CUDA graphs
    # - "eager": Standard PyTorch execution (default, more compatible)
    # - "inductor": torch.compile with inductor backend (faster, less compatible)
    compiler: "eager"

  # ---------------------------------------------------------------------------
  # torch.compile Optimization (optional)
  # ---------------------------------------------------------------------------
  # Optimizes model with torch.compile for additional performance.
  # Can be combined with CUDA graphs for best performance.
  torch_compile:
    # Enable torch.compile optimization (experimental)
    # Provides additional speedup on top of CUDA graphs
    enabled: false

    # Enable debug mode for torch.compile
    # Useful for diagnosing compilation issues
    debug_mode: false

    # Maximum batch size for torch.compile optimization
    # torch.compile is only applied to batch sizes <= this value
    # Higher values = longer compilation time, more memory
    max_bs: 32

  # ---------------------------------------------------------------------------
  # Custom Logit Processor (optional, for advanced sampling control)
  # ---------------------------------------------------------------------------
  # Enables user-defined logit transformations for advanced sampling control.
  # Useful for RL action masking, reward shaping, or constrained decoding.
  custom_logit_processor:
    # Enable custom logit processor support
    # Must be enabled for custom_logit_processor in sampling_params to work
    # Disabled by default for security (processors execute arbitrary code)
    enabled: false

    # Built-in processors available when enabled:
    # - "disallowed_tokens": Mask specific token IDs (set logits to -inf)
    # - "thinking_budget": Control thinking token length (for reasoning models)
    # - "no_repeat_ngram": Prevent n-gram repetitions in sliding window
    #
    # RL USE CASE: Action masking for constrained action spaces
    # Create a processor that masks invalid actions at each step.
    # See ppo_base_config.yaml for full RLActionMaskProcessor example.
    #
    # To use custom processors:
    # 1. Create a class extending CustomLogitProcessor
    # 2. Serialize with processor.to_str()
    # 3. Pass serialized string in sampling_params.custom_logit_processor
    # 4. Pass per-request params in sampling_params.custom_params

  # ---------------------------------------------------------------------------
  # Disaggregated Prefill/Decode (optional, for high QPS scaling)
  # ---------------------------------------------------------------------------
  # Separate prefill and decode phases across different GPU workers for
  # improved throughput at scale (100k+ QPS). Prefill workers process
  # input prompts while decode workers handle token generation.
  disaggregation:
    # Disaggregation mode
    # - "null": Disabled (default, standard unified execution)
    # - "prefill": This instance handles prefill only
    # - "decode": This instance handles decode only
    mode: "null"

    # Transfer backend for KV cache communication between workers
    # - "mooncake": High-performance RDMA-based transfer (recommended)
    # - "nixl": NixL architecture transfer backend
    transfer_backend: "mooncake"

    # Bootstrap port for handshake and KV cache preallocation
    bootstrap_port: 8998

    # InfiniBand device for high-speed communication (null = auto-detect)
    ib_device: null

    # Decode worker configuration (when mode="prefill")
    decode:
      tp_size: null  # Tensor parallel size (null = same as prefill)
      dp_size: null  # Data parallel size (null = same as prefill)
      enable_offload_kvcache: false  # Offload KV cache to CPU when GPU full

    # Reserved tokens for decode KV cache
    num_reserved_decode_tokens: 512

    # Data Parallel optimizations for large batch sizes
    enable_dp_attention: false
    enable_dp_lm_head: false

  # ---------------------------------------------------------------------------
  # Prometheus Metrics and Observability (optional)
  # ---------------------------------------------------------------------------
  # Enable comprehensive observability for monitoring training performance
  metrics:
    enabled: false  # Set true to enable Prometheus metrics on /metrics endpoint

    # Enable metrics on all TP ranks (default: only TP rank 0)
    enable_for_all_schedulers: false

    # Latency histogram buckets (customize for your workload)
    # buckets:
    #   time_to_first_token: [0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
    #   inter_token_latency: [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]
    #   e2e_request_latency: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]

    # Token count histograms for analyzing request distributions
    collect_tokens_histogram: false

    # Per-request metrics export to files (for detailed analysis)
    # export_to_file:
    #   enabled: true
    #   directory: "/data/metrics"

    # OpenTelemetry distributed tracing
    # tracing:
    #   enabled: true
    #   otlp_endpoint: "localhost:4317"

    # Request logging (inputs, outputs, metadata)
    # logging:
    #   enabled: true
    #   level: 2        # 0=metadata, 1=+params, 2=+partial I/O, 3=full I/O
    #   format: "json"  # "text" or "json"

  # ---------------------------------------------------------------------------
  # Session-Based Generation (optional, for multi-turn KV cache reuse)
  # ---------------------------------------------------------------------------
  # Sessions enable efficient multi-turn RL by maintaining KV cache across turns.
  # Avoids redundant prefix recomputation, improving throughput 2-10x for multi-turn.
  #
  # When enabled with SkyRLGymGenerator + use_conversation_multi_turn=true:
  # - Session is automatically opened at start of each trajectory
  # - generate_with_session() is used instead of generate() for KV cache continuity
  # - Request IDs are tracked between turns for efficient prefix reuse
  # - Session is automatically closed at end of trajectory
  sessions:
    enabled: true  # ENABLED: 2-10x throughput improvement for multi-turn RL

    # KV cache capacity per session (in tokens)
    # Should be large enough for full conversation context
    default_capacity: 8192

    # Session pooling: reuse sessions across batches (RECOMMENDED)
    pool_sessions: true

    # Maximum sessions in pool (when pool_sessions=true)
    max_pool_size: 64

  # ---------------------------------------------------------------------------
  # Priority Scheduling and Request Preemption (SGLang only)
  # ---------------------------------------------------------------------------
  # Configure request scheduling policies and priority-based preemption.
  # Useful for prioritizing certain requests (e.g., reward model inference).
  scheduling:
    # Scheduling policy for request processing
    # - "fcfs": First-Come-First-Served (default, simple and fair)
    # - "lpm": Longest Prefix Match (optimizes prefix cache hit rate)
    # - "dfs-weight": Weighted DFS tree traversal (for complex prefix patterns)
    # - "lof": Longest Output First (prioritizes requests near completion)
    # - "random": Random selection (useful for load balancing)
    policy: "fcfs"

    # Enable priority-based request scheduling
    # When enabled, requests can specify priority values for ordering
    # Note: Only works with "fcfs" or "lof" policies
    # ENABLED: Prioritize reward model (priority=100) over policy (priority=10)
    enable_priority: true

    # Priority value interpretation
    # true: lower numeric values = higher priority (priority 0 = highest)
    # false: higher numeric values = higher priority
    low_priority_values_first: false

    # Preemption threshold for priority scheduling
    # A running request is preempted when a waiting request has
    # priority difference > threshold (lower = more aggressive preemption)
    preemption_threshold: 10

    # Schedule conservativeness (0.0 to 1.0+)
    # Lower = more aggressive scheduling (may cause memory pressure)
    # Higher = more conservative (better stability)
    conservativeness: 1.0

    # Chunked prefill configuration
    # Maximum tokens to prefill per chunk (enables incremental prefill)
    # Tuning Guidelines:
    #   - 2048-4096: Lower latency, smoother response times, better for interactive RL
    #   - 8192 (default): Balanced, good for most workloads
    #   - 16384+: Maximum throughput, higher latency variance
    # For RL training, 4096 often provides best balance of throughput and consistency
    # null = use SGLang defaults (typically 8192)
    chunked_prefill_size: null

    # REQUEST-LEVEL PRIORITY USAGE (RL reward model prioritization):
    # When enable_priority: true, pass priority per-request in sampling_params:
    #   sampling_params = {"priority": 0, "max_new_tokens": 1024, ...}
    # Use case: Prioritize reward model over policy generation during RL training.
    # Example: Reward requests with priority=0 (high), policy with priority=10.

  # ---------------------------------------------------------------------------
  # Hierarchical Cache (GPU↔CPU↔NVMe) - For Long Contexts
  # ---------------------------------------------------------------------------
  # Multi-tier KV cache with GPU, CPU RAM, and optional NVMe/disk storage.
  # Enables serving larger context lengths than GPU memory alone allows.
  # Useful for long-context RL environments (4x+ longer contexts).
  #
  # RECOMMENDATION: Enable for RL environments with >8K token contexts.
  # This allows 4x+ longer contexts than GPU memory alone.
  hierarchical_cache:
    # Enable hierarchical cache (GPU → CPU → Storage)
    # When enabled, KV cache can spill from GPU to CPU RAM and optionally to disk
    # Set to true for long-context RL (>8K tokens)
    enabled: false  # Set true for long-context RL environments

    # Host memory configuration
    host_memory:
      # Host/device cache size ratio (ratio=3.0 means CPU cache is 3x GPU cache)
      # RECOMMENDED: 3.0 for long-context RL
      ratio: 3.0  # TUNED: Up from 2.0 for long-context RL

      # Explicit host memory pool size in GB (overrides ratio if specified)
      size_gb: null

    # Write policy for cache hierarchy
    # - "write_through": Write to all tiers immediately (safe, slower)
    # - "write_back": Write to lower tier on eviction (fast)
    # - "write_through_selective": Selective based on access patterns
    write_policy: null  # Default: "write_through"

    # I/O backend for host memory operations
    # - "kernel": Standard kernel I/O (most compatible)
    # - "direct": Direct I/O bypassing page cache (lower latency)
    io_backend: null  # Default: "kernel"

    # Memory layout for cache organization
    # - "layer_first": Organize by layer (better for layer-parallel)
    # - "page_first": Organize by page (better for page-parallel)
    mem_layout: null  # Default: "layer_first"

    # Storage backend for Tier 3 (NVMe/disk) - optional
    storage:
      # Storage backend type (null = GPU + CPU only, no disk)
      # Options: "file" (NVMe/SSD), "mooncake", "nixl", "hf3fs"
      backend: null

      # Storage prefetch policy
      prefetch_policy: null  # Default: "best_effort"

  # ---------------------------------------------------------------------------
  # SGLang Engine Init Kwargs
  # Pass any SGLang-specific parameters here
  # ---------------------------------------------------------------------------
  engine_init_kwargs:
    # Attention backend
    attention_backend: "fa3"      # FlashAttention 3 (default)
    mm_attention_backend: "fa3"   # Multi-modal attention backend

    # LoRA settings (when enable_lora=true)
    # max_lora_rank: 64
    # max_loras_per_batch: 8
    # lora_backend: "csgmv"

    # MoE settings (for Mixture of Experts models)
    # moe_a2a_backend: "deepep"    # All-to-all backend
    # moe_runner_backend: "auto"   # Runner backend
    # enable_eplb: true            # Expert load balancing

# =============================================================================
# ROLLOUT CONFIGURATION
# =============================================================================
rollout:
  n_samples_per_prompt: 4  # Samples per prompt for GRPO/RLOO
  batch_size: 16           # Prompts per generation batch

# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================
env:
  name: "gsm8k"  # or your custom environment
  max_turns: 1   # Max conversation turns
